# -*- coding: utf-8 -*-
"""Pricing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zpRtJUloDhZQSbaELPepNwQjkCYZu408

# Leitura das bibliotecas
E dos datasets
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn

#verificando a versão do kelarn

# import sklearn

# !pip uninstall scikit-learn -y

# !pip install -U scikit-learn

dataset = pd.read_csv('/content/drive/MyDrive/train.csv')

dataset.info()

df = dataset.copy()
df.head()

"""# Separação Treino e Teste"""

from sklearn.model_selection import train_test_split
X = df.drop('SalePrice',axis=1)
y = df.SalePrice

X_tr,X_ts,y_tr,y_ts = train_test_split(X, y, test_size=0.33, random_state=42)

"""# Missing Values"""

null_values = X_tr.isnull().sum()[X_tr.isnull().sum() > 0]/len(X_tr)*100
null_values.sort_values(ascending= False)

"""Por terem tanto valores nulos, o que resta é pouco representativo para a base, vou excluir as seguintes colunas

**PoolQC**: Pool quality

**MiscFeature**: Miscellaneous feature not covered in other categories - Também não relevante

**Alley**: Type of alley access to property - Não relevante

"""

X_tr_o = X_tr.copy()
X_ts_o = X_ts.copy()

X_tr_o = X_tr_o.drop(columns=['PoolQC','MiscFeature','Alley'])
X_ts_o = X_ts_o.drop(columns=['PoolQC','MiscFeature','Alley'])
null_values = X_tr_o.isnull().sum()[X_tr_o.isnull().sum() > 0]/len(X_tr_o)*100
null_values.sort_values(ascending= False)

"""As seguintes variáveis já possuem uma significante amostragem de valores não nulos. Faz sentido preencher a ausência por -1 nesses casos

**FireplaceQu**: Fireplace quality

**Fence**: Fence quality


"""

X_tr_o[['FireplaceQu','Fence']] = X_tr_o[['FireplaceQu','Fence']].fillna('Inexist')
X_tr_o[['FireplaceQu','Fence']]

"""**LotFrontage**: Linear feet of street connected to property"""

X_tr_o.LotFrontage.describe()

"""Comparar o preço médio da casa com LotFrontage nulo, com os normais"""

print('Preço médio nulo: ',y_tr[X_tr_o.LotFrontage.isna()].mean())
print('Preço médio n/nulo: ',y_tr[X_tr_o.LotFrontage.isna() == False].mean())
print('Diferença: ',(y_tr[X_tr_o.LotFrontage.isna()].mean()-y_tr[X_tr_o.LotFrontage.isna() == False].mean())/y_tr.mean()*100,'%')

"""É possível ver que os valores, por serem nulos possuem um impacto na variável resposta, dessa forma vou tratar como um valor particular -1"""

plt.figure(figsize=(10,5))
plt.hist(y_tr[X_tr_o.LotFrontage.isna()], rwidth=0.9,bins=30, density=True,color= 'b', alpha=0.5)
plt.hist(y_tr[X_tr_o.LotFrontage.isna() == False], rwidth=0.9,bins=30, density=True,color= 'r', alpha=0.5);

X_tr_o.LotFrontage = X_tr_o.LotFrontage.fillna(-1)
X_ts_o.LotFrontage = X_ts_o.LotFrontage.fillna(-1)

null_values = X_tr_o.isnull().sum()[X_tr_o.isnull().sum() > 0]/len(X_tr_o)*100
null_values.sort_values(ascending= False)

"""Tratamento das variáveis relacionadas à garagem: Como os nulos são casas sem garagem, irei especificar em meu modelo

**GarageType**: Garage location
		
**GarageYrBlt**: Year garage was built
		
**GarageFinish**: Interior finish of the garage

**GarageCars**: Size of garage in car capacity - Altamente correlacionada com car GarageArea

**GarageArea**: Size of garage in square feet - Altamente correlacionada com car GarageCars

**GarageQual**: Garage quality

**GarageCond**: Garage condition

"""

garage_cols = X_tr_o.columns[(X_tr_o.columns).str.contains('Garage')]
for col,dtypes in X_tr_o[garage_cols].dtypes.items():
  if dtypes == 'object':
    X_tr_o[col] = X_tr_o[col].fillna('Inexist')
    X_ts_o[col] = X_ts_o[col].fillna('Inexist')
  else:
    X_tr_o[col] = X_tr_o[col].fillna(-1)
    X_ts_o[col] = X_ts_o[col].fillna(-1)
X_tr_o

"""O resto possui pouco parte dos dados como valores nulos, irei exclui-los"""

null_rows_tr = X_tr_o[X_tr_o.isnull().any(axis=1)].index
null_rows_ts = X_ts_o[X_ts_o.isnull().any(axis=1)].index

X_tr_o = X_tr_o.drop(null_rows_tr)
y_tr = y_tr.drop(null_rows_tr)

X_ts_o = X_ts_o.drop(null_rows_ts)
y_ts = y_ts.drop(null_rows_ts)

null_values = X_tr_o.isnull().sum()[X_tr_o.isnull().sum() > 0]/len(X_tr_o)*100
null_values.sort_values(ascending= False)

"""# Standalizing"""

from sklearn.preprocessing import RobustScaler

X_tr_std = X_tr_o.copy()
X_ts_std = X_ts_o.copy()


scalers = {}
for col,dtypes in X_tr_std.dtypes.items():
  if dtypes != 'object':
    scaler = RobustScaler()
    scaler.fit(X_tr_std[[col]])
    X_tr_std.loc[:,col] = scaler.transform(X_tr_std[[col]])
    X_ts_std.loc[:,col] = scaler.transform(X_ts_std[[col]])
    scalers[col] = scaler

X_tr_std

"""# Encoding

Muitas das variáveis são ordinais, ou seja, que tenha uma sequência lógica. Elas extão padronizadas em:

       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor

Vou fazer o encoding com base nisso.
"""

from sklearn.preprocessing import OrdinalEncoder
X_tr_enc = X_tr_std.copy()
X_ts_enc = X_ts_std.copy()

encoder = {}
default_cat = ['Po', 'Fa', 'TA', 'Gd', 'Ex']
for col,dtype in X_tr_enc.dtypes.items():
    if dtype == 'object':
      encoder[col] = OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value= -1,categories=[default_cat])
      X_tr_enc.loc[:,col] = encoder[col].fit_transform(X_tr_enc[[col]],)
      X_ts_enc.loc[:,col] = encoder[col].transform(X_ts_enc[[col]])

X_tr_enc

"""# Correlação

O dataset é composto de variáveis categóricas e númericas, por isso é necessário fazer um tratamento especifico para cada par de variáveis de tipos diferentes.

"""

X_tr_o.info()

categoric_cols = X_tr_o.select_dtypes(include='object')
numeric_cols = X_tr_o.select_dtypes(exclude='object')
categoric_cols

"""Será feito o seguinte tratamento para os tipos de variáveis, entre: 

- Númericas e categóricas: **Correlation_Ratio**

- Categóricas: **Cramers-V**

- Númericas: Persons


"""

def correlation_ratio(categories, measurements):
    fcat, _ = pd.factorize(categories.reset_index(drop=True))
    cat_num = np.max(fcat)+1
    y_avg_array = np.zeros(cat_num)
    n_array = np.zeros(cat_num)
    for i in range(0,cat_num):
        cat_measures = measurements.reset_index(drop=True)[np.argwhere(fcat == i).flatten()]
        n_array[i] = len(cat_measures)
        y_avg_array[i] = np.average(cat_measures)
    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)
    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))
    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))
    if numerator == 0:
        eta = 0.0
    else:
        eta = np.sqrt(numerator/denominator)
    return eta


import scipy.stats as ss
def cramers_v(x, y):
  confusion_matrix = pd.crosstab(x,y)
  chi2 = ss.chi2_contingency(confusion_matrix)[0]
  n = confusion_matrix.sum().sum()
  phi2 = chi2/n
  r,k = confusion_matrix.shape
  phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))
  rcorr = r-((r-1)**2)/(n-1)
  kcorr = k-((k-1)**2)/(n-1)
  return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))

cor_df = pd.DataFrame(columns=X_tr_o.columns,index=X_tr_o.columns)
cor_num_vs_cat = pd.DataFrame(columns=numeric_cols.columns,index=categoric_cols.columns)
cor_num_vs_num = pd.DataFrame(columns=numeric_cols.columns,index=numeric_cols.columns)
cor_cat_vs_cat = pd.DataFrame(columns=categoric_cols.columns,index=categoric_cols.columns)
cols_analysed = []

for col,dtypes_col in X_tr_o.dtypes.items():
  for index,dtypes_idx in X_tr_o.dtypes.items():
    if ([col,index] in cols_analysed) or ([index,col] in cols_analysed):
      
      continue
    else:
      #Entre uma categorica e uma numerica
      if (dtypes_col == 'object' and dtypes_idx != 'object') or (dtypes_idx == 'object' and dtypes_col != 'object'):

        if (dtypes_col == 'object' and dtypes_idx != 'object'):
          cor = correlation_ratio(X_tr_o[col], X_tr_o[index])
          cor_num_vs_cat.loc[col,index] = cor
        elif (dtypes_idx == 'object' and dtypes_col != 'object'):
          cor = correlation_ratio(X_tr_o[index], X_tr_o[col])
        
          cor_num_vs_cat.loc[index,col] = cor

      #Entre duas categóricas
      elif dtypes_col == 'object' and dtypes_idx == 'object':
        cor = cramers_v(X_tr_o[col], X_tr_o[index])
        cor_cat_vs_cat.loc[index,col] = cor
      
      #Entre duas numéricas
      else:
        cor = X_tr_o[col].corr(X_tr_o[index],method='pearson')
        cor_num_vs_num.loc[index,col] = cor

      cols_analysed.append([col,index])
      cor_df.loc[index,col] = cor

cor_df = cor_df.astype('float64')
cor_cat_vs_cat = cor_cat_vs_cat.astype('float64')
cor_num_vs_cat = cor_num_vs_cat.astype('float64')
cor_num_vs_num = cor_num_vs_num.astype('float64')
cor_df

"""### Correlação entre as variáveis categóricas

Usarei aqui método **Cramers-V**, em que a correlação varia de 0 (nada correlacionada) à 1 (inteiramente correlacionada). Ou seja, não possui valores negativos como a correlação que já conhecemos
"""

plt.figure(figsize=(20,20))
sn.heatmap(cor_cat_vs_cat.round(6),annot=True, square=True, fmt='.2f',annot_kws={'size':9},  cmap= "coolwarm")

"""## Correlação entre variáveis continuas e categóricas

Correlation Ratio - O cálculo baseia-se na comparação entre a dipersão da variável númerica Y dado uma categoria X e a dispersão total de Y da população total. 
![image.png](data:image/png;base64,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)
"""

plt.figure(figsize=(20,20))
sn.heatmap(cor_num_vs_cat.round(6),annot=True, square=True, fmt='.2f',annot_kws={'size':9},  cmap= "coolwarm")

"""## Correlação entre variáveis contínuas"""

plt.figure(figsize=(20,20))
sn.heatmap(cor_num_vs_num.round(6),annot=True, square=True, fmt='.2f',annot_kws={'size':9},  cmap= "coolwarm")

"""## Excluindo as colunas correlacionadas

Aqui usarei o função mutual_info_regression do sklearn. O cálculo consiste basicamente na estimação de entropia nas distancias dos k-vizinhos.
Não usei o Teste F para testar a covariância neste caso, pois este teste só reconhece depedência linear, enquanto o mutual_info_regression reconhece qualquer tipo de dependência.

Fonte: https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html
"""

from sklearn.feature_selection import mutual_info_regression

X_tr_corr = X_tr_enc.copy()
X_ts_corr = X_ts_enc.copy()


dropped = []

for num1,col1 in enumerate(cor_df.columns):
    if col1 in dropped: 
        continue # Pule se a coluna 1 já foi dropada
    for num2,col2 in enumerate(cor_df.index):
        if col1 in dropped:
            break # Pare se a coluna 1 já foi dropada
        if col2 in dropped:
            continue # Continue se a coluna 2 já foi dropada
        if num2 <= num1: 
            continue # Continue se estou olhando para a diagonal inferior da matriz
        if col1==col2:
            continue # Continue se estou olhando para a diagonal da matriz. Pois sempre serão terão corr == 1
        val = cor_df.loc[col2,col1]
        if abs(val) < .7:  # Continue se o valor da correção entre col2 e col1 foi inferior a esse
            continue
            
        
        #mutual_info_classif(y_train.values.reshape(-1,1),y_train)y_train_enc
        imm1 = mutual_info_regression(X_tr_corr[[col1]],np.ravel(y_tr))
        imm2 = mutual_info_regression(X_tr_corr[[col2]],np.ravel(y_tr))
        
        if imm1 > imm2:
            dropped.append(col2.strip())
            print(f'Entre {col1} e {col2}, descartamos {col2}')
        else:
            dropped.append(col1.strip())
            print(f'Entre {col1} e {col2}, descartamos {col1}')


X_tr_corr = X_tr_corr.drop(dropped, axis=1)
X_ts_corr = X_ts_corr.drop(dropped, axis=1)

plt.plot(X_tr_o.groupby('YearBuilt').EnclosedPorch.mean())

plt.figure(figsize=(10,5))
plt.plot(X_tr_o['YearBuilt'],X_tr_o['Neighborhood'],'.')
plt.xticks(rotation=90);

"""# Feature Selection

Para seleção das variáveis mais importantes
"""

def normal(x,m,s):
    return 1/(np.sqrt(2*np.pi)*s) * np.exp((-1/2) * ((x-m)/s)**2)

plt.figure(figsize=(10,5))
plt.hist(y_tr, bins =list(range(y_tr.min(),y_tr.max(),20000)),rwidth=0.9,density=True)
plt.plot(np.linspace(y_tr_out.min(),y_tr_out.max(),20000), normal(np.linspace(y_tr_out.min(),y_tr_out.max(),20000),y_tr_out.mean(), y_tr_out.std(ddof=1)),
         'k-', lw=3);

normal(np.linspace(y_tr_out.min(),y_tr_out.max(),20000),y_tr_out.mean(), y_tr_out.std(ddof=1))

"""## LASSO LARS"""

from sklearn.linear_model import LassoLarsCV

reg = LassoLarsCV(fit_intercept=True,cv=5, normalize=True)
reg.fit(X_tr_corr, y_tr)

"""### Avaliação"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_pred = reg.predict(X_ts_corr)


print('MAE: ',mean_absolute_error(y_ts, y_pred),mean_absolute_error(y_ts, y_pred)/y_pred.mean())
print('MSE: ',mean_squared_error(y_ts, y_pred,squared=False),mean_squared_error(y_ts, y_pred)/y_pred.mean())


# Com correlação
# MAE:  27008.727607765464 0.152595257462053
# MSE:  60359.56226907547 20583.966888838564

# SEM CORRELAÇÃO
# MAE:  26408.487546878576 0.1491584807576586
# MSE:  59057.82929072074 19699.689558256006

"""Existe uma GRANDE diferença entre as métricas MAE E MSE. Isso geralmente se da por conta de outliers na variável resposta.

Vamos dar uma olhada...

### Coeficientes
"""

df_coefs = pd.DataFrame(np.column_stack((X_tr_corr.columns,reg.coef_)),columns=['features','coefs'])
only_important = df_coefs[df_coefs.coefs.abs() > 0]
only_important.sort_values('coefs', ascending=False)

"""### Análise dos erros

Os erros não estão se distribuindo de forma normal. Provavelmente umproblema de outliers
"""

def normal(x,m,s):
    return 1/(np.sqrt(2*np.pi)*s) * np.exp((-1/2) * ((x-m)/s)**2)

plt.figure(figsize=(15,5))
res = y_pred-y_ts
plt.hist(res, bins=50, rwidth=.8, density=True)
plt.plot(np.linspace(res.min(),res.max(),100), normal(np.linspace(res.min(),res.max(),100),res.mean(), res.std(ddof=1)), 'k-', lw=3);

"""# Regressão Linear
Com as features relevantes
"""

from sklearn.linear_model import LinearRegression
column = only_important.features.unique()
lr = LinearRegression(normalize=True, fit_intercept=True, positive=True)
lr.fit(X_tr_corr[column], y_tr)

"""### Avaliação"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_pred = lr.predict(X_ts_corr[column])


print('MAE: ',mean_absolute_error(y_ts, y_pred))
print('RMSE: ',mean_squared_error(y_ts, y_pred,squared=False))

# SEM CORRELAÇÃO
# MAE:  31988.153602093367
# RMSE:  66011.31829987536

# COM CORRELAÇÃO
# MAE:  29935.304771453517
# RMSE:  62839.579917926145

"""### Coeficientes

Melhorou consideralvemente os erros. Agora que estamos com os meus betas ótimos para minimização dos erros, vamos analisa-los
"""

df_coefs_lr = pd.DataFrame(np.column_stack((column,lr.coef_)),columns=['features','coefs'])

df_coefs_lr.sort_values('coefs', ascending=False)

"""A variável que mais impacta positivamente é a **OverallQual**	o que faz todo o sentido. Quanto melhor a qualidade em geral, maior o preço.
A variável que mais impactou negativamente no preço foi a **ExterCond**. Por que a quanto melhor a condição externa pior o preço? Precisamos entrar no detalhe

#### ExterCond
ExterCond - valuates the present condition of the material on the exterior

"""

df.ExterCond.value_counts(dropna=False)

"""Podemos identificar que os preços sobem conforme a qualidade externa melhora, como deveria. A questão é por que essa variáveil ta afetando negativamente a variável resposta"""

plt.figure(figsize=(10,5))
sn.swarmplot(x = y_tr,y = X_tr_o['ExterCond'],order=default_cat[::-1]);

default_cat

plt.figure(figsize=(10,5))
plt.bar('Po',df[df.ExterCond == 'Po'].SalePrice.mean())
plt.bar('Fa',df[df.ExterCond == 'Fa'].SalePrice.mean())
plt.bar('TA',df[df.ExterCond == 'TA'].SalePrice.mean())
plt.bar('Gd',df[df.ExterCond == 'Gd'].SalePrice.mean())
plt.bar('Ex',df[df.ExterCond == 'Ex'].SalePrice.mean());

"""A média pode ser impactada pelos outliers, mas dependendo da amostragem pode variar menos. Vamos ver como está distribuição desses outliers dentre as categorias"""

import seaborn as sn
plt.figure(figsize=(10,5))
sn.boxplot(data = df,x='ExterCond',y='SalePrice',order=default_cat)

"""Interessate! A categoria TA, possui as casos com os maiores valores do dataset, pode ser por isso que está fazendo essa variável ter correlação negativa com a resposta.

#### GarageCond
O mesmo acontece com GarageCondition
"""

X_tr.GarageCond.value_counts().loc[default_cat]

plt.figure(figsize=(10,5))
sn.boxplot(data = pd.concat([X_tr_enc,y_tr],axis=1),x='GarageCond',y='SalePrice')

"""Comparando com uma variável que está correlacionada, e que modelo retornou que tem beta positivo."""

X_tr.GarageQual.value_counts().loc[default_cat]

plt.figure(figsize=(10,5))
sn.boxplot(data = df,x='GarageQual',y='SalePrice',order=default_cat)

"""## Modeling"""

from sklearn.linear_model import LinearRegression
column = only_important.features.unique()
lr_o = LinearRegression(normalize=True, fit_intercept=True, positive=True)
lr_o.fit(X_tr_corr[column], y_tr)

"""## Avaliação Final"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_pred_out = lr_o.predict(X_ts_corr[column])

mae_metrics_out = round(mean_absolute_error(y_ts, y_pred_out),2)
mse_metrics_out = round(mean_squared_error(y_ts, y_pred_out,squared=False),2)

print('MAE sem outliers: ', mae_metrics_out)
print('MSE sem outliers: ', mse_metrics_out)


# SEM CORRELAÇÂO
# MAE sem outliers:  23354.31
# MSE sem outliers:  28555.3

# Com Correlação
# MAE sem outliers:  22142.16
# MSE sem outliers:  26572.59

"""Quando comparado ao modelo em que foi treinado com os outilers. Vemos que houve uma melhora significativa"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_pred_out = lr.predict(X_ts_corr[column])

mae_metrics = round(mean_absolute_error(y_ts, y_pred_out),2)
mse_metrics = round(mean_squared_error(y_ts, y_pred_out,squared=False),2)

print('MAE: ', mae_metrics)
print('MSE: ', mse_metrics)

# SEM CORRELAÇÂO
# MAE:  23482.49
# MSE:  29226.32


# COM Correlação
# MAE:  21677.74
# MSE:  25753.41

def normal(x,m,s):
    return 1/(np.sqrt(2*np.pi)*s) * np.exp((-1/2) * ((x-m)/s)**2)

plt.figure(figsize=(15,5))
res = y_pred_out-y_ts
plt.hist(res, bins=100, rwidth=.8, density=True)
plt.plot(np.linspace(res.min(),res.max(),100), normal(np.linspace(res.min(),res.max(),100),res.mean(), res.std(ddof=1)), 'k-', lw=3);

"""Talvez vale tratarmos manualmente esses casos de outliers

# Outliers
"""

y_tr.describe()

"""O cálculo para identificação desses valores é através da 1,5 * FIQ (Interquartil) do conjunto de TREINO e aplicado ao conjunto teste. """

outlier_max = y_tr.quantile(0.75) + 1.5*(y_tr.quantile(0.75) - y_tr.quantile(0.25))
outlier_idx = y_tr[y_tr > outlier_max].index

"""## Anomaly detection

A idéia é fazer um modelo de classificação para identificar outliers na base.

Lembrando que o outlier foi calculado através do método 1.5 FIQ. O limite entre uma observação normal e um outliers é:
"""

outlier_max

"""Temos uma total de 3% da variável dependente considerado outliter"""

y_tr_classif = y_tr.apply(lambda x:1 if x >= outlier_max else 0)
y_ts_classif = y_ts.apply(lambda x:1 if x >= outlier_max else 0)
y_tr_classif.mean()

"""### Ordinal Encode"""

from sklearn.preprocessing import OrdinalEncoder
X_tr_enc_anom = X_tr_o.copy()
X_ts_enc_anom = X_ts_o.copy()

encoder = {}
default_cat = ['Po', 'Fa', 'TA', 'Gd', 'Ex']
for col,dtype in X_tr_enc_anom.dtypes.items():
    if dtype == 'object':
      encoder[col] = OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value= -1,categories=[default_cat])
      X_tr_enc_anom.loc[:,col] = encoder[col].fit_transform(X_tr_enc_anom[[col]],)
      X_ts_enc_anom.loc[:,col] = encoder[col].transform(X_ts_enc_anom[[col]])

X_tr_enc_anom

"""### Otimização dos Hiperparêmtros"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV


params = {
    'criterion': ['gini','entropy'],
    'max_depth': [7,9,11,13,15],   
}


rf = GridSearchCV(
    RandomForestClassifier(n_estimators= 100,n_jobs= 4),
    params,
    cv=5,
    scoring= 'roc_auc',
    verbose=10
)

rf.fit(X_tr_enc_anom,y_tr_classif)

rf.best_estimator_

"""### Predição e avaliação """

from sklearn.metrics import roc_auc_score

y_pred_ts = rf.predict(X_ts_enc_anom)
y_pred_tr = rf.predict(X_tr_enc_anom)

roc_auc_score(y_ts_classif,y_pred)

"""Aparentemete os dados estão sendo separados perfeitamente. Talvez exista um padrão para os preços dos outliers na base"""

plt.figure(figsize=(10,5))
plt.hist(y_pred_ts[y_ts_classif == 1],bins=30,rwidth=0.9,alpha=0.6,density=True)
plt.hist(y_pred_ts[y_ts_classif == 0],bins=30,rwidth=0.9,alpha=0.6,density=True);

from sklearn.metrics import recall_score


roc_auc_score(y_ts_classif,y_pred)

feature_importance = pd.DataFrame(np.column_stack((X_ts_enc_anom.columns,rf.best_estimator_.feature_importances_)),columns=['columns','importance'])
feature_importance[feature_importance.importance > 0].sort_values('importance',ascending=False)

X_tr_corr['outlier'] = y_pred_tr
X_ts_corr['outlier'] = y_pred_ts
X_tr_corr

"""# Treinando o modelo
Com a coluna de previsão de outliers
"""

from sklearn.linear_model import LinearRegression
column = only_important.features.unique()
lr_anom = LinearRegression(normalize=True, fit_intercept=True, positive=True)
lr_anom.fit(X_tr_corr[np.append(column,'outlier')], y_tr)

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_pred_anom = lr_anom.predict(X_ts_corr[np.append(column,'outlier')])


print('MAE: ',mean_absolute_error(y_ts, y_pred_anom))
print('RMSE: ',mean_squared_error(y_ts, y_pred_anom,squared=False))

RMSE_anom = mean_squared_error(y_ts, y_pred_anom,squared=False)
RMSE = mse_metrics_out

print(f'A redução nos erros foi de {round((RMSE_anom - RMSE)/RMSE*100,2)}% para RMSE')

"""### Análise dos residuos"""



"""#### Conjunto Treino"""

plt.figure(figsize=(15,5))
y_pred_tr = lr_anom.predict(X_tr_corr[np.append(column,'outlier')])
res = y_pred_tr-y_tr
plt.hist(res, bins=100, rwidth=.8, density=True)
plt.plot(np.linspace(res.min(),res.max(),100), normal(np.linspace(res.min(),res.max(),100),res.mean(), res.std(ddof=1)), 'k-', lw=3);

"""#### Conjunto teste"""

plt.figure(figsize=(15,5))
res = y_pred-y_ts
plt.hist(res, bins=100, rwidth=.8, density=True)
plt.plot(np.linspace(res.min(),res.max(),100), normal(np.linspace(res.min(),res.max(),100),res.mean(), res.std(ddof=1)), 'k-', lw=3);

y_tr_log = np.log(y_tr)
plt.hist(y_tr_log)

"""## Log transformation"""

from sklearn.compose import TransformedTargetRegressor
regressor = LinearRegression(fit_intercept=True, positive=True)

tt = TransformedTargetRegressor(regressor=regressor,func=np.log, inverse_func=np.exp)
tt.fit(X_tr_corr[np.append(column,'outlier')], y_tr)

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_pred_tt = tt.predict(X_ts_corr[np.append(column,'outlier')])


print('MAE: ',mean_absolute_error(y_ts, y_pred_tt))
print('RMSE: ',mean_squared_error(y_ts, y_pred_tt,squared=False))

# MAE:  21582.27531774511
# RMSE:  36029.38259098918

from sklearn.metrics import r2_score

y_pred_tt = tt.predict(X_ts_corr[np.append(column,'outlier')])

r2_score(y_ts,y_pred_tt)

plt.figure(figsize=(15,5))
res = y_pred_tt-y_ts
plt.hist(res, bins=100, rwidth=.8, density=True)
plt.plot(np.linspace(res.min(),res.max(),100), normal(np.linspace(res.min(),res.max(),100),res.mean(), res.std(ddof=1)), 'k-', lw=3);

print(y_ts[res < -100_000],y_pred_anom[res < -100_000])

pd.Series(y_pred_anom).sort_values(ascending=False)

df_coefs_lr = pd.DataFrame(np.column_stack((np.append(column,'outlier'),tt.regressor_.coef_)),columns=['features','coefs'])

df_coefs_lr.sort_values('coefs', ascending=False)







cramers_v(df1.MSZoning,df1.Street)



